{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2016/may/13/newsfeed-saga-unmasks-the-human-face-of-facebook", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2016-05-13T21:44:45Z", "webTitle": "Facebook's news saga reminds us humans are biased by design", "webUrl": "https://www.theguardian.com/technology/2016/may/13/newsfeed-saga-unmasks-the-human-face-of-facebook", "apiUrl": "https://content.guardianapis.com/technology/2016/may/13/newsfeed-saga-unmasks-the-human-face-of-facebook", "fields": {"headline": "Facebook's news saga reminds us humans are biased by design", "bodyText": "On Thursday, Facebook finally admitted what many had suspected for years: its \u201ctrending topics\u201d feature, a small box that lives on the top right of its website, is edited by journalists, relying as much on their news sense and good old-fashioned editorial guidelines as on Facebook\u2019s own technological solutions to the problem of finding \u201ctrending\u201d stories. The journalists involved in curating Facebook\u2019s trending topics feature have reportedly spent much of their two years on the job forced into silence by strict non-disclosure agreements preventing them from even revealing that they worked at the company. But a report from US technology site Gizmodo this month began a process that would eventually see the social network admit that Facebook was more than a mere platform. The tipping point was a follow-up report, alleging that its curators \u201croutinely suppressed conservative news\u201d. The report, citing an anonymous ex-employee, said that the company didn\u2019t cover trending stories that had been reported by rightwing sites, such as Breitbart or Newsmax, until a more mainstream publication, such as the New York Times or BBC, had picked them up. Facebook denied the story, but that wasn\u2019t enough to stop a furore engulfing the highest levels of the company. The US Senate commerce committee\u2019s chair, John Thune, called for a congressional inquiry into the company, and Facebook\u2019s chief executive, Mark Zuckerberg, announced plans to invite \u201cleading conservatives\u201d to talk with him about accusations of political bias. On Thursday, documents obtained by the Guardian revealed that Facebook\u2019s denial was probably justified. In a series of documents laying out the editorial standards for the trending topics section, the company dictated to its journalists how they should deal with news reports. It did indeed suggest that a story shouldn\u2019t be marked as important if it only came from an outlet such as Newsmax, but not for political reasons. Instead, the company expressed a preference for stories from a list of 10 large, mainstream political news sites, including Fox News, the Guardian and the BBC. \u201cI believe that what happened with these curators is that they wanted to avoid embarrassing mistakes,\u201d said Jay Rosen, professor of journalism at New York University. \u201cParticularly because they had no real power at the company, they had no authority. And so they wanted to only pick reliable news. \u201cThe easiest way to do that \u2013 not a particularly smart way \u2013 but the easiest way is to only pick things that are being published by major news organisations. In an environment like that, a story that is only circulating on rightwing blogs and conservative news sites may in fact be left out for reasons of bureaucracy more than bias.\u201d The introduction of human hands in the selection of trending topics dates back to the summer of 2014, when two different stories were taking social media by storm. On Facebook, the Ice Bucket Challenge was taking over newsfeeds, proving the perfect type of content to ride to viral success, but on Twitter, a platform barely mediated at all, the Black Lives Matter protests were dominating the reaction to the shooting of teenager Michael Brown in Ferguson. Some worried that algorithmically curated newsfeeds could never fully capture important breaking news events, and it seems Facebook was convinced: the curation team was set up, and guidelines put in place that allowed it to override the algorithm if enough major publications were reporting a story. Shortly after the Guardian published Facebook\u2019s guidelines, the company itself released a later version of them, and Justin Osofsky, its vice-president of global operations, said: \u201cThe guidelines demonstrate that we have a series of checks and balances in place to help surface the most important popular stories, regardless of where they fall on the ideological spectrum.\u201d While the publication of the company\u2019s editorial guidelines may serve to dampen the fire being kindled in the conservative blogosphere, in the long run it could open Facebook up to deeper criticism. Rosen said: \u201cFacebook is a company with an editorial role, editorial power, but no editorial culture or leadership. You can\u2019t call the editor-in-chief of Facebook and ask them what\u2019s going on, because there is no editor-in-chief. There is no ombudsman.\u201d And while Facebook has opened up about how it puts together its trending topics, the company remains taciturn about its main newsfeed, the feature that algorithmically pulls together a selection of posts to present to each individual user. Those algorithms are, however, still designed by humans. The lack of any editorial ombudsman, Rosen argues, means that \u201cthey don\u2019t have any way to answer questions about the priorities that go into newsfeeds. They occasionally release information about it: occasionally they\u2019ll put little updates on the blog \u2026 But there\u2019s no systematic way for them to reveal what their editorial judgment is, because they\u2019re not at the point where they can really say to themselves: \u2018We make editorial judgments.\u2019\u201d"}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}