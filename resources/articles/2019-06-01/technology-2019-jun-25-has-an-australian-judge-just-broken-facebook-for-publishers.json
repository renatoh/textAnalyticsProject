{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2019/jun/25/has-an-australian-judge-just-broken-facebook-for-publishers", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2019-06-25T09:50:44Z", "webTitle": "Has an Australian judge just broken Facebook for publishers?", "webUrl": "https://www.theguardian.com/technology/2019/jun/25/has-an-australian-judge-just-broken-facebook-for-publishers", "apiUrl": "https://content.guardianapis.com/technology/2019/jun/25/has-an-australian-judge-just-broken-facebook-for-publishers", "fields": {"headline": "Has an Australian judge just broken Facebook for publishers?", "bodyText": "A landmark ruling in the New South Wales supreme court has again raised concerns about defamation law in Australia, and has the potential to significantly affect the way media companies engage with the social media giant Facebook. Judge Stephen Rothman this week found media companies could be regarded as the publishers of comments made on Facebook, and as such have a responsibility to ensure defamatory remarks are not posted in the first place. It effectively means news organisations have a responsibility to pre-moderate comments on Facebook, significantly increasing the burden of using the social media site and creating a new challenge in grappling with Australia\u2019s defamation laws. What did the ruling say? Judge Rothman\u2019s ruling was part of an ongoing defamation trial brought by Dylan Voller against a number of media companies including News Corp and Channel Nine. Voller, whose treatment as a detainee inside the Don Dale Youth Detention Centre in the Northern Territory triggered a royal commission in 2016, is suing the Australian, the Sydney Morning Herald and the Centralian Advocate newspapers, as well as Sky News Australia\u2019s the Bolt Report. The action relates not to the articles themselves, but to comments made about Voller by members of the public on 10 Facebook posts published on the companies\u2019 public Facebook pages in 2016 and 2017, which he alleges carried false and defamatory imputations. While media companies were already liable for Facebook comments made on articles posted on their public pages, previously the test related to whether a publisher had been negligent in not removing potentially defamatory comments. For example, in Duffy v Google in 2015 the South Australian supreme court found the search engine was liable for defamatory material about Dr Janice Duffy appearing in its search results. The court found that once Google was alerted to the defamatory material, it was then under an obligation to act to censor its search results and prevent further harm to Duffy\u2019s reputation. The ruling was upheld by the full court on appeal in 2017. But Voller had not alleged the media companies were negligent or reckless in failing to delete the comments, and instead Rothman found there was a responsibility to moderate comments pre-publication. He found that media companies engaged with Facebook because of a commercial imperative \u2013 increased reader engagement through the social media platform allows publishers to charge more for advertising \u2013 and as such assumed the risk of potentially defamatory comments. Rothman also found that it was possible for media companies to essentially pre-moderate comments by using filters which included pronouns, definite and indefinite articles, and all conjunctions and prepositions. \u201cThe judge recognised there is no in-built system on Facebook that allows you to prevent the publication of comments on Facebook prior to it becoming public,\u201d Paul Gordon, a social media lawyer from Wallmans Lawyers in Adelaide said. \u201cBut he also found it would be possible to create a filter using commonly used words like \u2018and\u2019, \u2018he\u2019, \u2018she\u2019, \u2018but\u2019, in order to capture most posts. And that\u2019s what he said media organisations should be doing, duct-taping together a filter using common words.\u201d Who does it affect? According to Gordon, Rothman\u2019s ruling is not necessarily limited to just media companies. The ruling could potentially impact on any business operating a public Facebook page with a commercial motivation, depending in part on the content itself. \u201cI think it can extend to any company that runs a public Facebook page,\u201d he said. \u201cThe key element here though was that news outlets not only publish controversial content, but do so for the purpose of gaining commentary. But if someone on a company\u2019s Facebook page posted something particularly controversial, I don\u2019t see why it wouldn\u2019t apply. There\u2019s no part of the law or common law which says it only applies to media companies.\u201d Does the ruling apply retrospectively? Yes. A common law ruling doesn\u2019t change the law, it merely sets a precedent for how the courts should interpret an existing law. In general terms, defamation action in Australia can\u2019t be brought after one year from \u201cthe date of the publication of the matter complained of\u201d. But that definition is complicated by online publications. Unlike other jurisdictions including the UK, Australia does not have a \u201csingle publication rule\u201d, which would limit a defamation action to the date of the first publication. It means that in relation to online publications, it\u2019s possible for people suing for defamation to argue that an article is a \u201ccontinuing publication\u201d if it is still available online, meaning the limitation period is effectively open-ended. Does it include personal Facebook pages? Gordon thinks that\u2019s unclear. Rothman\u2019s judgment makes some comment about the technical differences between a public and a private Facebook page for the purposes of defamatory comments. But Gordon says the judgment \u201cdoesn\u2019t come to a conclusion in a meaningful way\u201d about defamatory comments on a private page. \u201cThe finding here is effectively that the media companies, by posting a controversial story to a public page which was providing them with a commercial benefit, were effectively encouraging the publication of defamatory material,\u201d he said. \u201cThere could be an argument made that if you made the post in a personal capacity the same argument should apply, but the judgment doesn\u2019t go into that in any great deal.\u201d So, who\u2019s liable if, for example, someone makes a defamatory comment on an article posted in a large private Facebook group? The commenter, the poster of the article, or the administrator? \u201cIt\u2019s probably all three,\u201d Gordon says. \u201cThe judge did say the third parties (that is, the commenters) in this case, if the content is defamatory, are certainly all publishers,\u201d he said. \u201cIt\u2019s not just the media organisations that are liable here, it\u2019s just that they are the ones with the money.\u201d Does it extend to other social media platforms? Probably not. The finding was \u201cquite Facebook centric\u201d, Gordon says, particularly because of its focus on the capacity for media companies to pre-moderate comments by imposing restrictive word filters. \u201cThe judge came to his decision because there was this mechanism which allowed the media organisations to create a de facto filter,\u201d he said. \u201cIf there are other social media platforms that do have the filter it could become relevant however.\u201d"}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}