{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "australia-news/commentisfree/2019/dec/06/facebook-is-part-of-a-toxic-ecosystem-of-hate-it-should-be-regulated-or-shut-down", "type": "article", "sectionId": "australia-news", "sectionName": "Australia news", "webPublicationDate": "2019-12-06T02:38:13Z", "webTitle": "Facebook is part of a toxic ecosystem of hate \u2013 it should be regulated or shut down | Jason Wilson", "webUrl": "https://www.theguardian.com/australia-news/commentisfree/2019/dec/06/facebook-is-part-of-a-toxic-ecosystem-of-hate-it-should-be-regulated-or-shut-down", "apiUrl": "https://content.guardianapis.com/australia-news/commentisfree/2019/dec/06/facebook-is-part-of-a-toxic-ecosystem-of-hate-it-should-be-regulated-or-shut-down", "fields": {"headline": "Facebook is part of a toxic ecosystem of hate \u2013 it should be regulated or shut down", "bodyText": "An investigation by the Guardian reveals that an online network has found a way to make money by pushing a steady stream of low-grade rightwing propaganda at low-information users around the world. We might argue, then, that what they created was a miniature version of Facebook\u2019s own business. The investigation suggests that several deceptive operators engaged in a \u201ccovert plot to control some of Facebook\u2019s largest far right pages\u201d. But it appears they did so not for ideological reasons, but because they understood it as a way to make cash by directing ordinary users to amateurish websites dripping with ads. While under their control, the pages stepped up racist attacks on Muslims \u2014 especially female Muslim politicians like Ilhan Omar in the US and Mehreen Faruqi in Australia. They pilloried others, like Jeremy Corbyn and Justin Trudeau, who were depicted as capitulating to Islamist terror. But evidence uncovered by the investigation suggests it was less that these were true believers than that they understood that racism begets rage, which in turn leads to clicks and cash. The scheme worked in part because of the segment of users it was targeted at, those whose buttons are easily pressed. In the words of Timothy Graham, a QUT researcher interviewed for the investigation, habitu\u00e9s of such pages are prone to consume and, importantly, share \u201ccontent that is highly emotive and contains polarising and extreme material\u201d. Their willingness to smash the share button means that they are \u201cgreat for business\u201d. The retreat of tolerance; the disintegration of liberal democracies into warring, hostile camps; the flood of death threats that high profile Muslim legislators of colour receive \u2014 these are mere externalities from the point of view of the entrepreneurs of hate. Even if it\u2019s possible to clean up the mess they made, they likely won\u2019t be responsible for doing it. In all of these respects the con artists exhibited the same kind of moral bankruptcy that has led Facebook itself through a string of scandals. Facebook may not actively desire the fragmentation of polities, the rising wave of extremism and the performance of pogroms. But enraged users are engaged users, and the company\u2019s profits depend on allowing space for hate to flourish. Facebook has allowed unscrupulous companies to harvest user data for micro-targeted ads, for purposes including election interference. Facebook and other platforms it owns, like WhatsApp, were the platform of choice for those involved in genocide in Myanmar and lynchings in India. Facebook and its family of apps still provide key platforms for fascists in the west to propagandise, proselytise, and organise. But the company is, in practical terms, unrepentant. Facebook can\u2019t, won\u2019t and never will voluntarily engage in the necessary amount of proactive moderation which would inhibit hate, even if, as in this case, the enterprise appeared to be a flagrant violation of its stated rules. Facebook\u2019s profit margins depend on keeping moderation costs low, and engagement high. Its existing moderators are reportedly overworked and underpaid. Mark Zuckerberg\u2019s public moves and private meetings suggest he is much more concerned about disingenuous rightwing claims about tech censorship than he is about the platform\u2019s incubation of hate speech. Even the political pressure arising from the various scandals have not made Facebook stop and reconsider. In response to the Guardian story, Facebook has performed a now-familiar ritual. It has shut some of the pages down, and made the same mollifying noises it offers whenever journalists discover a new vector for hate speech, or a new racket on the site. But as in many similar instances in the past, the damage is done. The death threats can\u2019t be taken back, polarisation cannot easily be reversed, and open Islamophobia has become a little more normalised. We shouldn\u2019t be fooled by Facebook\u2019s rote apologies. We should expect that, absent some significant changes, the company will continue to offer space for hatred to be expressed, and even monetised on its platforms. We should bear in mind the extent to which Facebook itself has monetised hatred and even violence. We should understand that Facebook will never effectively regulate itself. Given the history set out above, this is outrageous. But that outrage should lead us to consider drastic remedies. In polities whose gridlock and polarisation is in significant part a consequence of the rise of social media, we must nevertheless find a democratic means to regulate Facebook and other social media companies. It\u2019s not an exaggeration to say that democracy itself may be at stake. If we can\u2019t find a way to regulate it effectively, we might consider a course of action that others, like Vox\u2019s Matt Yglesias have counselled, and turn the website off."}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}