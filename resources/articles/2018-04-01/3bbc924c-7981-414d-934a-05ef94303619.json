{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2018/apr/24/facebook-releases-content-moderation-guidelines-secret-rules", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2018-04-24T22:14:38Z", "webTitle": "Facebook releases content moderation guidelines \u2013 rules long kept secret", "webUrl": "https://www.theguardian.com/technology/2018/apr/24/facebook-releases-content-moderation-guidelines-secret-rules", "apiUrl": "https://content.guardianapis.com/technology/2018/apr/24/facebook-releases-content-moderation-guidelines-secret-rules", "fields": {"headline": "Facebook releases content moderation guidelines \u2013 rules long kept secret", "bodyText": "A year after the Guardian revealed Facebook\u2019s secret rules for content moderation, the company has released a public version of its guidelines for what is and is not allowed on the site, and for the first time created a process for individuals to appeal censorship decisions. The disclosure comes amid a publicity blitz by the company to regain users\u2019 trust following the Observer\u2019s revelation in March that the personal Facebook data of tens of millions of users was improperly obtained by a political consultancy. In a Facebook post on Tuesday, the company\u2019s chief executive, Mark Zuckerberg, said that the publication of the guidelines was a step toward his personal goal \u201cto develop a more democratic and independent system for determining Facebook\u2019s Community Standards\u201d. The 27-page document provides insight into the rationale Facebook uses to develop its standards, which must balance users\u2019 safety and the right to free expression while maintaining a sufficiently sanitized platform for advertisers. The company has long banned content from terrorist organizations and hate groups, for example, but the document reveals how Facebook actually defines such groups. \u201cYou have told us that you don\u2019t understand our policies; it\u2019s our responsibility to provide clarity,\u201d said Monika Bickert, Facebook\u2019s vice-president of global policy management, in a blogpost. Other details include bans on posts in which an individual admits to using non-medical drugs \u201cunless posted in a recovery context\u201d, posts that compare a private individual to \u201canimals that are culturally perceived as intellectually or physically inferior or to an inanimate object\u201d, and images that include the \u201cpresence of by-products of sexual activity\u201d unless the content is intended to be satirical or humorous. One guideline bans images that include a \u201cvisible anus and/or fully nude close-ups of buttocks unless photoshopped on a public figure\u201d. Spam messages are not allowed to \u201cpromise non-existent Facebook features\u201d. The company also bans so-called \u201cfalse flag\u201d assertions (eg claiming that the victims of a violent tragedy are lying or are paid actors), defining such statements as harassment. Facebook\u2019s role as a global publisher \u2013 and censor \u2013 has long created controversy for the platform, and free speech advocates have for years advocated for more transparency from Facebook around its decision-making process for content takedowns. Sarah T Roberts, a UCLA assistant professor of information studies, said: \u201c[The disclosure] shows in no uncertain terms the great power these platforms have in terms of shaping people\u2019s information consumption and how people formulate their points of view \u2013 and how important it is to understand the internal mechanisms at play.\u201d But the document leaves out the kind of specific examples of policy enforcement that the Guardian published in the Facebook Files. Those documents \u2013 which were used for training content moderators \u2013 revealed how complicated it is to apply seemingly straightforward standards. According to training slides on hate speech, for example, the statements \u201cYou are such a Jew\u201d or \u201cMigrants are so filthy\u201d are allowed, but writing \u201cIrish are the best, but really French sucks\u201d is not. The release of Facebook\u2019s rules draws attention to the difficult work of the 7,500 content moderators \u2013 many of them subcontractors \u2013 who are tasked with applying the rules to the billions of pieces of content uploaded to the platform. The low-paid job involves being exposed to the most graphic and extreme content on the internet, making quick judgments about whether a certain symbol is linked to a terrorist group or whether a nude drawing is satirical, educational, or merely salacious. \u201cNow that we know about the policies, what can we know about the people who can enforce them?\u201d said Roberts. \u201cHow can we be assured they have the support and information they need to make decisions on behalf of the platform and all of us?\u201d While speech advocates have long called on Facebook to provide some form of appeals process for content removal, many said Tuesday that the company\u2019s current plan was inadequate. The new plan, which the company said would be built upon over the next year, will allow users whose posts were removed for nudity, sexual activity, hate speech or violence to request having their posts re-reviewed by a human moderator. \u201cUsers need a meaningful, robust right to appeal the removal of any post \u2013 and before it is removed,\u201d Nicole Ozer, the director of technology and civil liberties for the ACLU, said in a statement. She called on Facebook to provide a process whereby users can explain why their content should not be censored; she also urged Facebook to release statistics about its content removals. Ozer also raised concerns about Facebook\u2019s reliance on artificial intelligence to enforce content rules. \u201cAI will not solve these problems,\u201d she said. \u201cIt will likely exacerbate them.\u201d"}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}