{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/blog/2008/mar/24/stoppingpeertopeerbandwidth", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2008-03-24T18:37:22Z", "webTitle": "Stopping peer-to-peer bandwidth hogs from ripping off the rest of us", "webUrl": "https://www.theguardian.com/technology/blog/2008/mar/24/stoppingpeertopeerbandwidth", "apiUrl": "https://content.guardianapis.com/technology/blog/2008/mar/24/stoppingpeertopeerbandwidth", "fields": {"headline": "Stopping peer-to-peer bandwidth hogs from ripping off the rest of us", "bodyText": "Internet service providers have a problem with the 10% of serious peer-to-peer file sharers who hog around 75% of the internet's bandwidth, making it perform significantly worse for the rest of us. At the moment, ISPs typically use \"traffic shaping\" between about 5pm and 11pm, which basically slugs the net for everyone, but at least the net still works. A better approach would be to change the \"fairness algorithm\" that lets P2P users grab an unfair share. According to Fixing the unfairness of TCP congestion control, a post by George Ou at ZD Net: Bob Briscoe (Chief researcher at the BT Network Research Centre) is on a mission to tackle one of the biggest problems facing the Internet. He wants the world to know that TCP (Transmission Control Protocol) congestion control is fundamentally broken and he has a proposal for the IETF to fix the root cause of the problem. The basic idea is to change the current AIMD (Additive Increase Multiplicative Decrease) fairness algorithm, which is abused by P2P and some other programs, to a system where all users get roughly the same bandwidth (or what they've paid for) whether they open one TCP stream or 1,000. Apparently this wouldn't make P2P downloads take any longer, but by making them less abusive, it would make the net perform better for everyone. In any case, according to Briscoe: Controlling relative flow rates alone is a completely impractical way of going about the problem. To be realistic for large-scale Internet deployment, relative flow rates should be the outcome of another fairness mechanism, not the mechanism itself. That other mechanism should share out the 'cost' of one user's actions on others -- how much each user's transfers restrict other transfers, given capacity constraints. Then flow rates will depend on a deeper level of fairness that has so far remained unnamed in the literature, but is best termed 'cost fairness'. Briscoe presented his idea in a paper, Flow rate fairness: Dismantling a religion, to the IETF in July, 2007. It's worth a read because it doesn't pull any punches, describing the current system as \"completely daft\"."}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}