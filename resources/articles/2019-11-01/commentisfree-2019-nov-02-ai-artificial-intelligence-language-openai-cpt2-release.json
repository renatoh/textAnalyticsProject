{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "commentisfree/2019/nov/02/ai-artificial-intelligence-language-openai-cpt2-release", "type": "article", "sectionId": "commentisfree", "sectionName": "Opinion", "webPublicationDate": "2019-11-02T16:00:09Z", "webTitle": "AI is making literary leaps \u2013 now we need the rules to catch up", "webUrl": "https://www.theguardian.com/commentisfree/2019/nov/02/ai-artificial-intelligence-language-openai-cpt2-release", "apiUrl": "https://content.guardianapis.com/commentisfree/2019/nov/02/ai-artificial-intelligence-language-openai-cpt2-release", "fields": {"headline": "AI is making literary leaps \u2013 now we need the rules to catch up", "bodyText": "Last February, OpenAI, an artificial intelligence research group based in San Francisco, announced that it has been training an AI language model called GPT-2, and that it now \u201cgenerates coherent paragraphs of text, achieves state-of-the-art performance on many language-modelling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarisation \u2013 all without task-specific training\u201d. If true, this would be a big deal. But, said OpenAI, \u201cdue to our concerns about malicious applications of the technology, we are not releasing the trained model. As an experiment in responsible disclosure, we are instead releasing a much smaller model for researchers to experiment with, as well as a technical paper.\u201d Given that OpenAI describes itself as a research institute dedicated to \u201cdiscovering and enacting the path to safe artificial general intelligence\u201d, this cautious approach to releasing a potentially powerful and disruptive tool into the wild seemed appropriate. But it appears to have enraged many researchers in the AI field for whom \u201crelease early and release often\u201d is a kind of mantra. After all, without full disclosure \u2013 of program code, training dataset, neural network weights, etc \u2013 how could independent researchers decide whether the claims made by OpenAI about its system were valid? The replicability of experiments is a cornerstone of scientific method, so the fact that some academic fields may be experiencing a \u201creplication crisis\u201d (a large number of studies that prove difficult or impossible to reproduce) is worrying. We don\u2019t want the same to happen to AI. On the other hand, the world is now suffering the consequences of tech companies like Facebook, Google, Twitter, LinkedIn, Uber and co designing algorithms for increasing \u201cuser engagement\u201d and releasing them on an unsuspecting world with apparently no thought of their unintended consequences. And we now know that some AI technologies \u2013 for example generative adversarial networks \u2013 are being used to generate increasingly convincing deepfake videos. If the row over GPT-2 has had one useful outcome, it is a growing realisation that the AI research community needs to come up with an agreed set of norms about what constitutes responsible publication (and therefore release). At the moment, as Prof Rebecca Crootof points out in an illuminating analysis on the Lawfare blog, there is no agreement about AI researchers\u2019 publication obligations. And of all the proliferating \u201cethical\u201d AI guidelines, only a few entities explicitly acknowledge that there may be times when limited release is appropriate. At the moment, the law has little to say about any of this \u2013 so we\u2019re currently at the same stage as we were when governments first started thinking about regulating medicinal drugs. In the case of GPT-2, my hunch is that fears about its pathogenic propensities may be overdone \u2013 not because it doesn\u2019t work, but because humans have long experience of dealing with print fakery. Ever since Gutenberg, people have been printing falsehoods and purporting to be someone else. But over the centuries, we\u2019ve developed ways of spotting fakes. Accordingly, machine-generated text poses less of a problem than video deepfakes. GPT-2\u2019s capabilities are undoubtedly impressive, though. In a fascinating essay, I, Language Robot, the neuroscientist and writer Patrick House reports on his experience of working alongside OpenAI\u2019s language model \u2013 which produces style-matched prose to any written prompt that it\u2019s fed. It was, he recounts, an instructive exercise. \u201cThe language robot and I never wrote the same story,\u201d he recalls. \u201cIt became, eventually, a writing tool, both as a curator to new ideas and a splint for bad ones. When, after a few months, I lost access to it, my brain felt like it had been lesioned. I missed it. I had found myself jumping into its head, asking what it might write as one would a human writing partner, which made me wonder: should I give it the same theory-of-mind, inductive respect as I give to other thinking things? Could I even help but do otherwise?\u201d The most fascinating part of House\u2019s report deals with his idea of asking the robot the same questions that George Plimpton asked Ernest Hemingway in his famous Paris Review interview in 1958. In reply to the question whether it remembered the exact moment when it decided to be a writer, it replied: \u201cI don\u2019t remember ever wanting not to be a writer. Writing had always been my way out. My escape. When I was a little kid, my sister was always coming home from school with homework assignments and I knew if I got hers and mine done early I could spend the next day\u2019s lunch hour looking at books. It was the first time that I actually thought that being a writer might happen.\u201d And when asked what it did when it got stuck, it replied: \u201cIt\u2019s hard to say, but I would say that it would be better not to have a muse at all, to do all that writing you think is going to be good without any conscious control over what you do. It\u2019s better to be totally in charge of your own actions.\u201d At this point, the reader gets the eerie uncanny valley feeling: this is almost, but not quite, authentic. But the technology is getting there. Why, any day now it\u2019ll be writing newspaper columns. What I\u2019m reading The web ain\u2019t the world A good report by the Reuters Institute at the University of Oxford challenges conventional wisdom by finding that most people still get their news from offline sources. Culinary conditioning TheConversation.com has an intriguing essay \u2013 \u201cHow steak became manly and salads became feminine\u201d \u2013 by Yale historian Paul Freedman. It\u2019s a bot\u2019s world Renee DiResta has written an insightful piece on the algorithmic public sphere called \u201cThere are bots. Look around\u201d at Ribbonfarm.com."}, "isHosted": false, "pillarId": "pillar/opinion", "pillarName": "Opinion"}}}