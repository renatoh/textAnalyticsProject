{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "music/shortcuts/2019/sep/10/taylor-swift-threatened-to-sue-microsoft-over-its-racist-chatbot-tay", "type": "article", "sectionId": "music", "sectionName": "Music", "webPublicationDate": "2019-09-09T23:05:27Z", "webTitle": "Taylor Swift threatened to sue Microsoft over its racist chatbot Tay", "webUrl": "https://www.theguardian.com/music/shortcuts/2019/sep/10/taylor-swift-threatened-to-sue-microsoft-over-its-racist-chatbot-tay", "apiUrl": "https://content.guardianapis.com/music/shortcuts/2019/sep/10/taylor-swift-threatened-to-sue-microsoft-over-its-racist-chatbot-tay", "fields": {"headline": "Taylor Swift threatened to sue Microsoft over its racist chatbot Tay", "bodyText": "Taylor Swift has claimed ownership over many things. In 2015, she applied for trademarks for lyrics including \u201cthis sick beat\u201d and \u201cNice to meet you. Where you been?\u201d A few months later, she went further, trademarking the year of her birth, \u201c1989\u201d. We now know it didn\u2019t end there. A new book reveals that, a year later, Swift claimed ownership of the name Tay \u2013 and threatened to sue Microsoft for infringing it. In the spring of 2016, Microsoft announced plans to bring a chatbot it had developed for the Chinese market to the US. The chatbot, XiaoIce, was designed to have conversations on social media with teenagers and young adults. Users developed a genuine affinity for it, and would spend a quarter of an hour a day unloading their hopes and fears to a friendly, yet non-judgmental ear. The US version of the chatbot was to be called Tay. And that, according to Microsoft\u2019s president, Brad Smith, is where Swift\u2019s legal representatives got involved. \u201cI was on vacation when I made the mistake of looking at my phone during dinner,\u201d Smith writes in his forthcoming book, Tools and Weapons. \u201cAn email had just arrived from a Beverly Hills lawyer who introduced himself by telling me: \u2018We represent Taylor Swift, on whose behalf this is directed to you.\u2019 \u201cHe went on to state that \u2018the name Tay, as I\u2019m sure you must know, is closely associated with our client.\u2019 No, I actually didn\u2019t know, but the email nonetheless grabbed my attention. \u201cThe lawyer went on to argue that the use of the name Tay created a false and misleading association between the popular singer and our chatbot, and that it violated federal and state laws,\u201d Smith adds. Swift was, it turned out, right to be concerned. Not because the Tay chatbot grew to become as popular as XiaoIce, but because of what happened next: the chatbot was turned on and plugged into Twitter, where it promptly became a Nazi. Tay had been built to learn from the conversations it had, improving its speech by listening to what people said to it. Unfortunately, that meant that when what Smith describes as \u201ca small group of American pranksters\u201d began bombarding it with racist statements, Tay soon began repeating the exact same ideas at other interlocutors. \u201cBush did 9/11 and Hitler would have done a better job than the monkey we have now,\u201d it tweeted. \u201cWE\u2019RE GOING TO BUILD A WALL, AND MEXICO IS GOING TO PAY FOR IT,\u201d it added. Within 18 hours, Microsoft disconnected the bot from the Tay Twitter account and withdrew it from the market. The event, Smith writes, provided a lesson \u201cnot just about cross-cultural norms but about the need for stronger AI safeguards\u201d. But it did, at least, get Taylor Swift off its back."}, "isHosted": false, "pillarId": "pillar/arts", "pillarName": "Arts"}}}