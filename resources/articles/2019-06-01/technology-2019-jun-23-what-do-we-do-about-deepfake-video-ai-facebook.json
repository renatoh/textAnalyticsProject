{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2019/jun/23/what-do-we-do-about-deepfake-video-ai-facebook", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2019-06-23T08:00:42Z", "webTitle": "What do we do about deepfake video?", "webUrl": "https://www.theguardian.com/technology/2019/jun/23/what-do-we-do-about-deepfake-video-ai-facebook", "apiUrl": "https://content.guardianapis.com/technology/2019/jun/23/what-do-we-do-about-deepfake-video-ai-facebook", "fields": {"headline": "What do we do about deepfake video?", "bodyText": "There exist, on the internet, any number of videos that show people doing things they never did. Real people, real faces, close to photorealistic footage; entirely unreal events. These videos are called deepfakes, and they\u2019re made using a particular kind of AI. Inevitably enough, they began in porn \u2013 there is a thriving online market for celebrity faces superimposed on porn actors\u2019 bodies \u2013 but the reason we\u2019re talking about them now is that people are worried about their impact on our already fervid political debate. Those worries are real enough to prompt the British government and the US Congress to look at ways of regulating them. The video that kicked off the sudden concern last month was, in fact, not a deepfake at all. It was a good old-fashioned doctored video of Nancy Pelosi, the speaker of the US House of Representatives. There were no fancy AIs involved; the video had simply been slowed down to about 75% of its usual speed, and the pitch of her voice raised to keep it sounding natural. It could have been done 50 years ago. But it made her look convincingly drunk or incapable, and was shared millions of times across every platform, including by Rudi Giuliani \u2013 Donald Trump\u2019s lawyer and the former mayor of New York. It got people worrying about fake videos in general, and deepfakes in particular. Since the Pelosi video came out, a deepfake of Mark Zuckerberg apparently talking about how he has \u201ctotal control of billions of people\u2019s stolen data\u201d and how he \u201cowe[s] it all to Spectre\u201d, the product of a team of satirical artists, went viral as well. Last year, the Oscar-winning director Jordan Peele and his brother-in-law, BuzzFeed CEO Jonah Peretti, created a deepfake of Barack Obama apparently calling Trump a \u201ccomplete and utter dipshit\u201d to warn of the risks to public discourse. A lot of our fears about technology are overstated. For instance, despite worries about screen time and social media, in general, high-quality research shows that there\u2019s little evidence of it having a major impact on our mental health. Every generation has its techno-panic: video nasties, violent computer games, pulp novels. But, says Sandra Wachter, a professor in the law and ethics of AI at the Oxford Internet Institute, deepfakes might be a different matter. \u201cI can understand the public concern,\u201d she says. \u201cAny tech developing so quickly could have unforeseen and unintended consequences.\u201d It\u2019s not that fake videos or misinformation are new, but things are changing so fast, she says, that it\u2019s challenging our ability to keep up. \u201cThe sophisticated way in which fake information can be created, how fast it can be created, and how endlessly it can be disseminated is on a different level. In the past, I could have spread lies, but my range was limited.\u201d Here\u2019s how deepfakes work. They are the product of not one but two AI algorithms, which work together in something called a \u201cgenerative adversarial network\u201d, or Gan. The two algorithms are called the generator and the discriminator. Imagine a Gan that has been designed to create believable spam emails. The discriminator would be exactly the same as a real spam filter algorithm: it would simply sort all emails into either \u201cspam\u201d or \u201cnot spam\u201d. It would do that by being given a huge folder of emails, and determining which elements were most often associated with the ones it was told were spam: perhaps words like \u201cenlarger\u201d or \u201cpills\u201d or \u201can accident that wasn\u2019t your fault\u201d. That folder is its \u201ctraining set\u201d. Then, as new emails came in, it would give each one a rating based on how many of these features it detected: 60% likely to be spam, 90% likely, and so on. All emails above a certain threshold would go into the spam folder. The bigger its training set, the better it gets at establishing real from fake. But the generator algorithm works the other way. It takes that same dataset and uses it to build new emails that don\u2019t look like spam. It knows to avoid words like \u201cpenis\u201d or \u201cwon an iPad\u201d. And when it makes them, it puts them into the stream of data going through the discriminator. The two are in competition: if the discriminator is fooled, the generator \u201cwins\u201d; if it isn\u2019t, the discriminator \u201cwins\u201d. And either way, it\u2019s a new piece of data for the Gan. The discriminator gets better at telling fake from real, so the generator has to get better at creating the fakes. It is an arms race, a self-reinforcing cycle. This same system can be used for creating almost any digital product: spam emails, art, music \u2013 or, of course, videos. Gans are hugely powerful, says Christina Hitrova, a researcher in digital ethics at the Alan Turing Institute for AI, and have many interesting uses \u2013 they\u2019re not just for creating deepfakes. The photorealistic imaginary people at ThisPersonDoesNotExist.com are all created with Gans. Discriminatory algorithms (such as spam filters) can be improved by Gans creating ever better things to test them with. It can do amazing things with pictures, including sharpening up fuzzy ones or colourising black-and-white ones. \u201cScientists are also exploring using Gans to create virtual chemical molecules,\u201d says Hitrova, \u201cto speed up materials science and medical discoveries: you can generate new molecules and simulate them to see what they can do.\u201d Gans were only invented in 2014, but have already become one of the most exciting tools in AI. But they are widely available, easy to use, and increasingly sophisticated, able to create ever more believable videos. \u201cThere\u2019s some way to go before the fakes are undetectable,\u201d says Hitrova. \u201cFor instance, with CGI faces, they haven\u2019t quite perfected the generation of teeth or eyes that look natural. But this is changing, and I think it\u2019s important that we explore solutions \u2013 technological solutions, and digital literacy solutions, as well as policy solutions.\u201d With Gans, one technological solution presents itself immediately: simply use the discriminator to tell whether a given video is fake. But, says Hitrova, \u201cObviously that\u2019s going to feed into the fake generator to produce even better fakes.\u201d For instance, she says, one tool was able to identify deepfakes by looking at the pattern of blinking. But then the next generation will take that into account, and future discriminators will have to use something else. The arms race that goes on inside Gans will go on outside, as well. Other technological solutions include hashing \u2013 essentially a form of digital watermarking, giving a video file a short string of numbers which is lost if the video is tampered with \u2013 or, controversially, \u201cauthenticated alibis\u201d, wherein public figures constantly record where they are and what they\u2019re doing, so that if a deepfake circulates apparently showing them doing something they want to disprove, they can show what they were really doing. That idea has been tentatively floated by the AI law specialist Danielle Citron, but as Hitrova points out, it has \u201cdystopian\u201d implications. None of these solutions can entirely remove the risk of deepfakes. Some form of authentification may work to tell you that certain things are real, but what if someone wants to deny the reality of something real? If there had been deepfakes in 2016, says Hitrova, \u201cTrump could have said, \u2018I never said \u2018grab them by the pussy\u2019.\u201d Most would not have believed him \u2013 it came from Access Hollywood tapes and was confirmed by the show\u2019s presenter \u2013 but it would have given an excuse for people to doubt them. Education \u2013 critical thinking and digital literacy \u2013 will be important too. Finnish children score highly on their ability to spot fake news, a trait that is credited to the country\u2019s policy of teaching critical thinking skills at school. But that can only be part of the solution. For one thing, most of us are not at school. Even if the current generation of schoolchildren becomes more wary \u2013 as they naturally are anyway, having grown up with digital technology \u2013 their elders will remain less so, as can be seen in the case of British MPs being fooled by obvious fake tweets. \u201cOlder people are much less tech-savvy,\u201d says Hitrova. \u201cThey\u2019re much more likely to share something without fact-checking it.\u201d Wachter and Hitrova agree that some sort of regulatory framework will be necessary. Both the US and the UK are grappling with the idea. At the moment, in the US, social media platforms are not held responsible for their content. Congress is considering changing that, and making such immunity dependent on \u201creasonable moderation practices\u201d. Some sort of requirement to identify fake content has also been floated. Wachter says that something like copyright, by which people have the right for their face not to be used falsely, may be useful, but that by the time you\u2019ve taken down a deepfake, the reputational damage may already be done, so preemptive regulation is needed too. A European Commission report two weeks ago found that digital disinformation was rife in the recent European elections, and that platforms are failing to take steps to reduce it. Facebook, for instance, has entirely washed its hands of responsibility for fact-checking, saying that it will only take down fake videos after a third-party fact-checker has declared it to be false. Britain, though, is taking a more active role, says Hitrova. \u201cThe EU is using the threat of regulation to force platforms to self-regulate, which so far they have not,\u201d she says. \u201cBut the UK\u2019s recent online harms white paper and the Department for Digital, Culture, Media and Sport subcommittee [on disinformation, which has not yet reported but is expected to recommend regulation] show that the UK is really planning to regulate. It\u2019s an important moment; they\u2019ll be the first country in the world to do so, they\u2019ll have a lot of work \u2013 it\u2019s no simple task to balance fake news against the rights to parody and art and political commentary \u2013 but it\u2019s truly important work.\u201d Wachter agrees: \u201cThe sophistication of the technology calls for new types of law.\u201d In the past, as new forms of information and disinformation have arisen, society has developed antibodies to deal with them: few people would be fooled by first world war propaganda now. But, says Wachter, the world is changing so fast that we may not be able to develop those antibodies this time around \u2013 and even if we do, it could take years, and we have a real problem to sort out right now. \u201cMaybe in 10 years\u2019 time we\u2019ll look back at this stuff and wonder how anyone took it seriously, but we\u2019re not there now.\u201d \u2022 The AI Does Not Hate You by Tom Chivers is published by Weidenfeld &amp; Nicolson (\u00a316.99). To order a copy go to guardianbookshop.com. Free UK p&amp;p on all online orders over \u00a315"}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}