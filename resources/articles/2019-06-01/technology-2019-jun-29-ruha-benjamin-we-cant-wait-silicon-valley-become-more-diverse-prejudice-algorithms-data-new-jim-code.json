{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2019/jun/29/ruha-benjamin-we-cant-wait-silicon-valley-become-more-diverse-prejudice-algorithms-data-new-jim-code", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2019-06-29T14:00:52Z", "webTitle": "Ruha Benjamin: \u2018We definitely can\u2019t wait for Silicon Valley to become more diverse\u2019", "webUrl": "https://www.theguardian.com/technology/2019/jun/29/ruha-benjamin-we-cant-wait-silicon-valley-become-more-diverse-prejudice-algorithms-data-new-jim-code", "apiUrl": "https://content.guardianapis.com/technology/2019/jun/29/ruha-benjamin-we-cant-wait-silicon-valley-become-more-diverse-prejudice-algorithms-data-new-jim-code", "fields": {"headline": "Ruha Benjamin: \u2018We definitely can\u2019t wait for Silicon Valley to become more diverse\u2019", "bodyText": "Ruha Benjamin is an associate professor of African American studies at Princeton University, and lectures around the intersection of race, justice and technology. She founded the Just Data Lab, which aims to bring together activists, technologists and artists to reassess how data can be used for justice. Her latest book, Race After Technology, looks at how the design of technology can be discriminatory. Where did the motivation to write this book come from? It seems like we\u2019re looking to outsource decisions to technology, on the assumption that it\u2019s going to make better decisions than us. We\u2019re seeing this in almost every arena \u2013 healthcare, education, employment, finance \u2013 and it\u2019s hard to find a context which it hasn\u2019t penetrated. Something which really sparked my interest was a series of headlines and articles I saw which were all about a phenomenon dubbed \u201cracist robots\u201d. Then, as time went on, these articles and headlines became less surprised, and they started to say, of course, the robots are racist because they\u2019re designed in a society with these biases. The idea that software can have prejudice embedded in it is known as algorithmic bias \u2013 how does it amplify prejudice? Many of these automated systems are trying to identify and predict risk. So we have to look at how risk was assessed historically \u2013 whether a bank would extend a loan to someone, or if a judge would give someone a certain sentence. The decisions of the past are the input for how we teach software to make those decisions in the future. If we live in a society where police profile black and Latinx people, that affects the police data on who is likely to be a criminal. So you\u2019ll have these communities overrepresented in the data sets, which are then used to train algorithms to look for future crimes, or predict who\u2019s seen to be higher risk and lower risk. Are there other areas of society \u2013 such as housing or finance \u2013 where the use of automated systems has resulted in biased outcomes? Policing and the courts are getting a lot of attention, as they should. But there are other areas too, such as Amazon\u2019s own hiring algorithms, which discriminated against women applicants, even though gender wasn\u2019t listed on those r\u00e9sum\u00e9s. The training set used data about who already worked at Amazon. Sometimes, the more intelligent machine learning becomes, the more discriminatory it can be \u2013 so in that case, it was able to pick up gender cues based on other aspects of those r\u00e9sum\u00e9s, like their previous education or their experience. In your book, you assert that the treatment of black communities is an indication of what\u2019s to come for other communities more generally. How would you say this extends to technology? Thinking about how risk is racialised is one way into understanding how those systems can eventually be deployed against many more people, not just the initial target. This is one of the things we can see with these new digital scoring systems \u2013 these companies which don\u2019t just look at your personal riskiness, but also your social media and the people you\u2019re connected with. If someone you know has defaulted on a loan, that can affect you. So actually, incorporating and gathering more data can be even more harmful to people\u2019s lives. What role can legislation or regulation play in changing this direction? I\u2019m personally a little sceptical \u2013 the passing of a law can be a placeholder for much more significant progress because people prematurely celebrate, even if not much changes. But I do increasingly think that legislation has a role to play. Even if a particular law is just a regulation in a state, or one country in Europe, it can be very effective because if these companies want to roll out technologies universally, and then they find they have to change something up for a certain jurisdiction, it can then be an obstacle. Other elements, like state-level protections for whistleblowers are vital, because there has been retaliation against workers at these tech companies. Home DNA testing kits are increasingly popular, and genomics screening is more commonplace too. Are you concerned about how technologies are being used, and weaponised? We did an informal audit of three DNA testing companies when I was a postdoc at UCLA. The results we got back were completely different across the three companies, because of their own reference data. These companies have access to our data, which they can buy and sell to other companies, and there\u2019s really very few regulatory safeguards on how this is going to be used. The similarity between those technologies \u2013 the DNA testing kits, artificial intelligence, machine learning \u2013 is that the reference data shapes so much of the prediction. We have to question how it\u2019s put together, what individuals are being used as reference points. You call these systems the new Jim Code, because of how they perpetuate inequality. How do they build on the legacy of Jim Crow? So the original Jim Crow was about designing racial segregation, but it was really about maintaining status hierarchies. Many people look at high levels of segregation \u2013 which does still exist \u2013 and they rarely question how it was designed, but instead put it down to the stereotypes of lazy people who don\u2019t value education, and all kinds of narratives along those lines. When we think about the new Jim Code, I\u2019m referring to things like the investigation from ProPublica, where companies could tick boxes saying, you don\u2019t want black people to see the real estate where you advertise, on Facebook. [Earlier this year US courts ruled against this and other discriminatory ad targeting.] How aware do you think people are of issues around algorithmic bias, or automated systems? Since I started working on this book, a little over two years ago, I have seen a dramatic shift in the tone of the conversation around technology. Part of that has been spurred on by Facebook and Cambridge Analytica and the US election. More and more people are realising that this idea of big tech coming to save us, it\u2019s really been dismantled. Part of it is shifting from a kind of paranoia around technology to what my activist colleagues like to say: from paranoia to power. Could a more diverse workforce in Silicon Valley potentially provide a solution to these problems? More diversity in Silicon Valley is important, but won\u2019t automatically address algorithmic bias. Unless all those diverse people are empowered to challenge discriminatory design processes, diversity is a ruse. We need a complete overhaul of the larger accountability structures that shape tech development, and we definitely can\u2019t wait for Silicon Valley to become more diverse before implementing much stronger regulation and accountability. \u2022 Race After Technology by Ruha Benjamin is published by Polity Press (\u00a314.99). To order a copy go to guardianbookshop.com. Free UK p&amp;p on all online orders over \u00a315"}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}