{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "tv-and-radio/2019/mar/19/the-internets-dirtiest-secrets-review-the-human-toll-of-detoxifying-social-media", "type": "article", "sectionId": "tv-and-radio", "sectionName": "Television & radio", "webPublicationDate": "2019-03-19T22:25:32Z", "webTitle": "The Internet\u2019s Dirtiest Secrets review \u2013 the human toll of detoxifying social media", "webUrl": "https://www.theguardian.com/tv-and-radio/2019/mar/19/the-internets-dirtiest-secrets-review-the-human-toll-of-detoxifying-social-media", "apiUrl": "https://content.guardianapis.com/tv-and-radio/2019/mar/19/the-internets-dirtiest-secrets-review-the-human-toll-of-detoxifying-social-media", "fields": {"headline": "The Internet\u2019s Dirtiest Secrets review \u2013 the human toll of detoxifying social media", "bodyText": "One woman wanted to quit her job as a moderator for an unnamed tech company during training, after hearing descriptions of the content and images she was likely to see. Once she had started, she came across pictures of a six\u2011year-old girl having terrible things done to her and asked to leave. Her manager told her this was what she had signed up for and sent her back to work. Her story was preceded by footage from testimony before a committee on child abuse images and exploitation by Nicole Wong, then a legal adviser at Google. \u201cWe\u2019re doing the best we can,\u201d she said. We don\u2019t know the name of the woman haunted by images that still make her voice shake when she speaks of them. She is one of tens of thousands of moderators employed by companies in the Philippines, themselves hired by big tech firms, to purge social media platforms of the worst that humanity offers when you give it the chance. Like the rest of her colleagues, she could only speak without risk anonymously. The Internet\u2019s Dirtiest Secrets: The Cleaners (BBC Four) was an episode of the BBC\u2019s Storyville strand that masterfully wove together two indictments \u2013 personal and political \u2013 to damn the efforts of firms such as Google, Twitter and Facebook to protect the people who work for them and to damn the socio-political infrastructures that are still the best ways we have found to keep us safe. Let us turn to the human misery unleashed at the individual level first. The moderators who work for these third parties \u2013 their names are not revealed, nor which companies they work for \u2013 are based in Manila, 7,000 miles from Silicon Valley. Here, there are two main options for employment. You can sift through garbage, scrambling over colossal heaps of the stuff and selling whatever you have scavenged at the end of each day until your body sickens, or you can be paid to sift through the online equivalent every day until your soul dies. Some moderators have a target of deleting 25,000 posts a day from the thousands identified as questionable and sent to them for adjudication. \u201cIt damages your brain,\u201d said one. \u201cIt\u2019s like a virus in me, slowly penetrating my brain and changing the reactions of my body,\u201d said another. One moderator, who specialised in judging live videos of self-harm, hanged himself after asking three times to be transferred. You can try to skip or avert your eyes, but managers review samples of everyone\u2019s decisions to make sure they are correct. If your scorecard suffers, so will your chances of continued employment. On a macro level, the implications of a system where broadbrush guidelines composed by the companies must be applied after only seconds of consideration by people usually unfamiliar with the language or context of a video or an image are almost limitless. A painting of a naked, looming Donald Trump with a tiny penis was taken down. There is no allowance in the backrooms for art, satire or nuance; a guideline outlawing naked genitals is a guideline outlawing naked genitals, and people need their jobs. Syrian citizens\u2019 footage of civilian bombings, which helps charities track the violence and try to hold parties to account, is routinely taken down. Other decisions make firms more overtly complicit in upholding repressive regimes. Google, for example, blocks any YouTube video the Turkish government doesn\u2019t like from being shown in that country, rather than have the government block YouTube entirely. \u201cI didn\u2019t love that resolution,\u201d said Wong. \u201cBut that\u2019s where we got to.\u201d Polarisation, fragmentation, abuse and escalation are not byproducts of this new technology, as the former Google design ethicist Tristan Harris pointed out. They are the fuel of anything that depends on human engagement for survival and revenue. Outrage generates clicks. \u201cIt used to be that everyone was entitled to their own opinion,\u201d said Antonio Garc\u00eda Mart\u00ednez, a former product manager at Facebook. \u201cNow, everyone is entitled to their own set of facts.\u201d Facebook\u2019s filters will provide. This was an extended instalment of Storyville \u2013 85 minutes instead of the usual hour. It is always a dense hour, without sacrificing accessibility, but this allowed a deeper dive and time to introduce ideas about the ramifications of new technology that is still relatively unfamiliar to most of us. At its heart, though, lay an old, old question: who guards the guardians? Perhaps we ought to add a new question: who, in an era when companies big enough to operate as independent states abound, has the balls to do so? \u2022 This article was amended on 1 April 2019. An earlier version implied that Facebook owns YouTube. Rather it is owned by Google."}, "isHosted": false, "pillarId": "pillar/arts", "pillarName": "Arts"}}}