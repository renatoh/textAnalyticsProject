{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2018/aug/16/facebook-myanmar-failure-blundering-toddler", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2018-08-16T21:00:36Z", "webTitle": "Facebook's failure in Myanmar is the work of a blundering toddler", "webUrl": "https://www.theguardian.com/technology/2018/aug/16/facebook-myanmar-failure-blundering-toddler", "apiUrl": "https://content.guardianapis.com/technology/2018/aug/16/facebook-myanmar-failure-blundering-toddler", "fields": {"headline": "Facebook's failure in Myanmar is the work of a blundering toddler", "bodyText": "When Facebook invited journalists for a phone briefing on Tuesday evening to talk about its progress in tackling hate speech in Myanmar, it seemed like a proactive, well-intentioned move from a company that is typically fighting PR fires on several fronts. But the publication of a bombshell Reuters investigation on Wednesday morning suggested otherwise: the press briefing was an ass-covering exercise. This is the latest in a series of strategic mishaps as the social network blunders its way through the world like a giant, uncoordinated toddler that repeatedly soils its diaper and then wonders where the stench is coming from. It enters markets with wide-eyed innocence and a mission to \u201cbuild [and monetise] communities\u201d, but ends up tripping over democracies and landing in a pile of ethnic cleansing. Oopsie! Human rights groups and researchers have been warning Facebook that its platform was being used to spread misinformation and promote hatred of Muslims, particularly the Rohingya, since 2013. As its user base exploded to 18 million, so too did hate speech, but the company was slow to react and earlier this year found its platform accused by a UN investigator of fuelling anti-Muslim violence. The Australian journalist and researcher Aela Callan warned Facebook about the spread of anti-Rohingya posts on the platform in November 2013. She met with the company\u2019s most senior communications and policy executive, Elliott Schrage. He referred her to staff at Internet.org, the company\u2019s effort to connect the developing world, and a couple of Facebook employees who dealt with civil society groups. \u201cHe didn\u2019t connect me to anyone inside Facebook who could deal with the actual problem,\u201d she told Reuters. In mid-2014, after false rumours online about a Muslim man raping a Buddhist woman triggered deadly riots in the city of Mandalay, the Myanmar government requested a crisis meeting with Facebook. Facebook said that government representatives should send an email when they saw examples of dangerous false news and the company would review them. It took until April this year \u2013 four years later \u2013 for Mark Zuckerberg to tell Congress that Facebook would step up its efforts to block hate messages in Myanmar, saying \u201cwe need to ramp up our effort there dramatically\u201d. Since then it has deleted some known hate figures from the platform, but this week\u2019s Reuters investigation \u2013 which found more than 1,000 posts, images and videos attacking Myanmar\u2019s Muslims \u2013 shows there\u2019s a long way to go. A key issue that civil society groups focus on is Facebook\u2019s lack of Burmese-speaking content moderators. In early 2015, there were just two of them. Up until Wednesday of this week, Facebook has refused to reveal how many Burmese content reviewers it had hired since. \u201cWe\u2019re still not clear how many Myanmar-speaking reviewers Facebook has despite our repeated requests for transparency around this,\u201d said Ei Myat Noe Khin, the digital rights manager at Phandeeyar, a tech innovation lab in Myanmar. \u201cFacebook has been more proactive reaching out to civil society, but it\u2019s important they don\u2019t rely on civil society as an alternative to hiring more staff.\u201d On the call on Tuesday evening, Ellen Silver, Facebook\u2019s VP of operations, said the figure would be \u201cmisleading\u201d because some content such as nudity doesn\u2019t require local language expertise. Facebook had a change of heart on Wednesday, after Reuters revealed that the company had 60 reviewers who could speak Burmese. The company published a blogpost including this number, and pledged to hire 40 more by the end of the year. The company still has no office or staff in Myanmar. During the call, the company also said that one of the reasons it couldn\u2019t moderate content effectively was because users weren\u2019t using its reporting tools. This might have something to do with the fact that those reporting tools \u2013 including the text in drop-down menus attached to objectionable posts \u2013 were only translated into Burmese in late April/early May this year. Facebook is also upgrading its technological systems to proactively identify offending content. Civil society groups have been underwhelmed by results so far. \u201cTheir AI can\u2019t detect real hate speech and rumours. Mostly it detects just the words like \u2018Ma Ba Tha\u2019 [a Buddhist monk-led nationalist group] and \u2018Buddha religious\u2019 or something like that,\u201d said Myat Thu from Burma Monitor, a not-for-profit. \u201cIt\u2019s not the tracing root cause. Only Burmese content reviewers can know the local context.\u201d One policy change that sounds promising on the face of it, is deleting inaccurate or misleading information created or shared \u201cwith the purpose of contributing to or exacerbating violence or physical harm\u201d. Until last month, the company would only de-rank misinformation and only delete threats that were sufficiently specific to meet the company\u2019s credible violence threshold. It was launched first in Sri Lanka, where disinformation has also triggered a spate of mob violence, and is being rolled out to Myanmar. When the Guardian asked how the notoriously metrics-focused company would measure the success of the policy, the answer was characteristically mealy-mouthed: \u201cOur goal is to get better at identifying and removing abuses of our platform that spread hate and can contribute to offline violence or harm, so people in Myanmar can safely enjoy the benefits of connectivity.\u201d When pushed again to specify how it would measure this, a spokeswoman said \u201cthat\u2019s difficult\u201d. In spite of these problems, both Burma Monitor and Phandeeyar welcomed the additional focus on Myanmar after years of inaction. \u201cIt\u2019s really too early to talk of progress, but Facebook is showing more willingness to engage with the problem, which is positive,\u201d said Phandeeyar\u2019s Ei Myat Noe Khin. \u201cWe hope that Facebook is committed to solving these problems over the long term \u2013 not just focusing on short term Band-Aid solutions, while there is a high level of public scrutiny,\u201d she added."}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}