{"response": {"status": "ok", "userTier": "developer", "total": 1, "content": {"id": "technology/2019/dec/04/ex-facebook-worker-claims-disturbing-content-led-to-ptsd", "type": "article", "sectionId": "technology", "sectionName": "Technology", "webPublicationDate": "2019-12-04T17:00:05Z", "webTitle": "Ex-Facebook worker claims disturbing content led to PTSD", "webUrl": "https://www.theguardian.com/technology/2019/dec/04/ex-facebook-worker-claims-disturbing-content-led-to-ptsd", "apiUrl": "https://content.guardianapis.com/technology/2019/dec/04/ex-facebook-worker-claims-disturbing-content-led-to-ptsd", "fields": {"headline": "Ex-Facebook worker claims disturbing content led to PTSD", "bodyText": "A former Facebook moderator is suing the company, alleging that his work scouring the site of violent and obscene content caused his post-traumatic stress disorder. Chris Gray, who now works as a tour guide, is seeking damages from both Facebook Ireland and CPL, the contracting firm that directly employed him. The case, filed on Wednesday in the Irish high court in Dublin, is thought to be the first time a former moderator has taken the social network to court. According to court documents, Gray\u2019s work required him to review \u201capproximately a thousand tickets per night\u201d, initially focused on pornography, and later \u201con content that had been reported as being threatening, hateful, bullying, or otherwise dangerous\u201d. Two years on, a number of specific pieces of content remain \u201cparticularly marked\u201d in his memory, the legal writ says, including \u201ca video in which a woman wearing an abaya is seen being stoned to death\u201d, \u201ca video in which persons, who appear to be migrants in Libya, are tortured with molten metal\u201d, and \u201cvideo footage of dogs being cooked alive\u201d. Aggravating the trauma, Gray\u2019s lawyers argue, was the fact that the nature of the work required him to \u201cobsess\u201d over particular videos. Facebook and CPL \u201cvalued accuracy \u2026 above all else\u201d, and tracked whether individual calls were made correctly or not. But the system did not allow moderators \u201cto hold or skip a ticket pending a decision from above\u201d, requiring them instead to focus deeply on particular pieces of content, often featuring violent or upsetting material. The complaint details one video, for instance, \u201cwhich collaged various scenes of people dying in different accidents \u2026 set to a musical soundtrack. [Gray] had a long argument with the quality point of contact [a senior role] about whether the music meant that the person posting it was \u2018celebrating\u2019 or whether it just counted as disturbing content.\u201d Speaking to the Guardian before the case was filed, Gray said: \u201cYou would wake up and you\u2019re remembering the video of someone machine-gunning people in the Middle East somewhere, trying to think whether there was an Isis flag, and so whether it should be marked as terrorism-related or not. \u201cIt took me a year after I left to realise how much I\u2019d been affected by the job. I don\u2019t sleep well, I get in stupid arguments, have trouble focusing.\u201d Gray is being supported in his case by Foxglove, an international NGO that backs efforts to hold big tech to account through the legal system. Cori Crider, Foxglove\u2019s director, said: \u201cThe reason we\u2019ve got involved is that we think that social media factory floors are unsafe and need to be cleared up. In a decade we\u2019re going to look back on this as we did at meat packing plants at the turn of the century. \u201cFacebook\u2019s only going to pay attention to things when they know that they\u2019ve got a typhoon bearing down on them. What I\u2019d like to see is the moderators realising how much power they have if they just organise. Because let\u2019s face it, social media as we know it could not exist without the labour people like Chris provide.\u201d In October, leaked audio from a Facebook all-staff meeting revealed Mark Zuckerberg describing reports about poor working conditions in the company\u2019s moderation centres as \u201ca little overdramatic\u201d. \u201cFrom digging into them and understanding what\u2019s going on, it\u2019s not that most people are just looking at just terrible things all day long,\u201d Zuckerberg told employees. \u201cBut there are really bad things that people have to deal with, and making sure that people get the right counselling and space and ability to take breaks and get the mental-health support that they need is a really important thing. It\u2019s something we\u2019ve worked on for years and are always trying to probe and understand how we can do a better job to support that.\u201d A Facebook spokesperson said: \u201cWe are committed to providing support for those that review content for Facebook as we recognise that reviewing certain types of content can sometimes be difficult. Everyone who reviews content for Facebook goes through an in-depth, multi-week training program on our Community Standards and has access to extensive psychological support to ensure their wellbeing. This includes 24/7 on-site support with trained practitioners, an on-call service, and access to private healthcare from the first day of employment. We are also employing technical solutions to limit their exposure to graphic material as much as possible. This is an important issue, and we are committed to getting this right.\u201d CPL have been contacted for comment."}, "isHosted": false, "pillarId": "pillar/news", "pillarName": "News"}}}